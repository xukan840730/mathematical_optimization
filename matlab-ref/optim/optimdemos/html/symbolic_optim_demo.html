
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Using Symbolic Mathematics with Optimization Toolbox&#8482; Solvers</title><meta name="generator" content="MATLAB 7.10"><meta name="date" content="2010-01-05"><meta name="m-file" content="symbolic_optim_demo"><link rel="stylesheet" type="text/css" href="../../../matlab/demos/private/style.css"></head><body><div class="header"><div class="left"><a href="matlab:edit symbolic_optim_demo">Open symbolic_optim_demo.m in the Editor</a></div><div class="right"><a href="matlab:echodemo symbolic_optim_demo">Run in the Command Window</a></div></div><div class="content"><h1>Using Symbolic Mathematics with Optimization Toolbox&#8482; Solvers</h1><!--introduction--><p>Optimization Toolbox&#8482; solvers are usually more accurate and efficient when you supply gradients and Hessians of the objective and constraint functions. This demo shows how to use the Symbolic Math Toolbox&#8482; functions named <tt>jacobian</tt> and <tt>matlabFunction</tt> to provide these derivatives to optimization solvers.</p><p><b>Additional Requirements:</b></p><div><ul><li>Symbolic Math Toolbox</li></ul></div><p>There are several considerations in using symbolic calculations with optimization functions:</p><p>1. Optimization objective and constraint functions should be defined in terms of a vector, say <tt>x</tt>. However, symbolic variables are scalar or complex-valued, not vector-valued. This requires you to translate between vectors and scalars.</p><p>2. Optimization gradients, and sometimes Hessians, are supposed to be calculated within the body of the objective or constraint functions. This means that a symbolic gradient or Hessian has to be placed in the appropriate place in the objective or constraint function file or function handle.</p><p>3. Calculating gradients and Hessians symbolically can be time-consuming. Therefore you should perform this calculation only once, and generate code, via <tt>matlabFunction</tt>, to call during execution of the solver.</p><p>4. Evaluating symbolic expressions with the <tt>subs</tt> function is time-consuming. It is much more efficient to use <tt>matlabFunction</tt>.</p><p>5. <tt>matlabFunction</tt> generates code that depends on the orientation of input vectors. Since <tt>fmincon</tt> calls the objective function with column vectors, you must be careful to call <tt>matlabFunction</tt> with column vectors of symbolic variables.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">First Example: Unconstrained Minimization with Hessian</a></li><li><a href="#7">Second Example: Constrained Minimization Using the fmincon Interior-Point Algorithm</a></li><li><a href="#14">Cleaning Up Symbolic Variables</a></li></ul></div><h2>First Example: Unconstrained Minimization with Hessian<a name="1"></a></h2><p>The objective function to minimize is:</p><p><img src="symbolic_optim_demo_eq87848.png" alt="$$f(x_1, x_2) = \log\left (1 + 3 \left ( x_2 - (x_1^3 - x_1)&#xA;\right )^2 + (x_1 - 4/3)^2 \right ).$$"></p><p>This function is positive, with a unique minimum value of zero attained at <tt>x1</tt> = 4/3, <tt>x2</tt> =(4/3)^3 - 4/3 = 1.0370...</p><p>We write the independent variables as <tt>x1</tt> and <tt>x2</tt> because in this form they can be used as symbolic variables. As components of a vector <tt>x</tt> they would be written <tt>x(1)</tt> and <tt>x(2)</tt>. The function has a twisty valley as depicted in the plot below.</p><pre class="codeinput">syms <span class="string">x1</span> <span class="string">x2</span> <span class="string">real</span>
x = [x1;x2]; <span class="comment">% column vector of symbolic variables</span>
f = log(1 + 3*(x2 - (x1^3 - x1))^2 + (x1 - 4/3)^2);

ezsurfc(f,[-2 2])
view(127,38)
</pre><img vspace="5" hspace="5" src="symbolic_optim_demo_01.png" alt=""> <p>Compute the gradient and Hessian of f:</p><pre class="codeinput">gradf = jacobian(f,x).'; <span class="comment">% column gradf</span>
hessf = jacobian(gradf,x);
</pre><p>The <tt>fminunc</tt> solver expects to pass in a vector x, and, with the <tt>GradObj</tt> and <tt>Hessian</tt> options set to 'on', expects a list of three outputs: [f(x),gradf(x),hessf(x)]</p><p><tt>matlabFunction</tt> generates exactly this list of three outputs from a list of three inputs. Furthermore, using the <tt>vars</tt> option, <tt>matlabFunction</tt> accepts vector inputs.</p><pre class="codeinput">fh = matlabFunction(f,gradf,hessf,<span class="string">'vars'</span>,{x});
</pre><p>Now solve the minimization problem starting at the point [-1,2]:</p><pre class="codeinput">options = optimset(<span class="string">'GradObj'</span>,<span class="string">'on'</span>,<span class="string">'Hessian'</span>,<span class="string">'on'</span>, <span class="keyword">...</span>
    <span class="string">'Display'</span>,<span class="string">'final'</span>);
[xfinal fval exitflag output] = fminunc(fh,[-1;2],options)
</pre><pre class="codeoutput">
Local minimum possible.

fminunc stopped because the final change in function value relative to 
its initial value is less than the default value of the function tolerance.




xfinal =

   1.333332064357340
   1.037030117814729


fval =

    7.662315226723625e-012


exitflag =

     3


output = 

       iterations: 14
        funcCount: 15
     cgiterations: 11
    firstorderopt: 3.439061368233648e-005
        algorithm: 'large-scale: trust-region Newton'
          message: [1x472 char]

</pre><p>Compare this with the number of iterations using no gradient or Hessian information. This requires the medium-scale algorithm, obtained by setting the <tt>LargeScale</tt> option to <tt>'off'</tt>:</p><pre class="codeinput">options = optimset(<span class="string">'Display'</span>,<span class="string">'final'</span>,<span class="string">'LargeScale'</span>,<span class="string">'off'</span>);
fh2 = matlabFunction(f,<span class="string">'vars'</span>,{x});
<span class="comment">% fh2 = objective with no gradient or Hessian</span>
[xfinal fval exitflag output2] = fminunc(fh2,[-1;2],options)
</pre><pre class="codeoutput">
Local minimum found.

Optimization completed because the size of the gradient is less than
the default value of the function tolerance.




xfinal =

   1.333337991777384
   1.037057531377053


fval =

    2.198508042259553e-011


exitflag =

     1


output2 = 

       iterations: 18
        funcCount: 81
         stepsize: 1
    firstorderopt: 2.458683006702789e-006
        algorithm: 'medium-scale: Quasi-Newton line search'
          message: [1x438 char]

</pre><p>The number of iterations is lower when using gradients and Hessians, and there are dramatically fewer function evaluations:</p><pre class="codeinput">sprintf([<span class="string">'There were %d iterations using gradient'</span> <span class="keyword">...</span>
    <span class="string">' and Hessian, but %d without them.'</span>], <span class="keyword">...</span>
    output.iterations,output2.iterations)
sprintf([<span class="string">'There were %d function evaluations using gradient'</span> <span class="keyword">...</span>
    <span class="string">' and Hessian, but %d without them.'</span>], <span class="keyword">...</span>
    output.funcCount,output2.funcCount)
</pre><pre class="codeoutput">
ans =

There were 14 iterations using gradient and Hessian, but 18 without them.


ans =

There were 15 function evaluations using gradient and Hessian, but 81 without them.

</pre><h2>Second Example: Constrained Minimization Using the fmincon Interior-Point Algorithm<a name="7"></a></h2><p>We consider the same objective function and starting point, but now have two nonlinear constraints:</p><p><img src="symbolic_optim_demo_eq26522.png" alt="$$5\sinh(x_2/5) \ge x_1^4$$"></p><p><img src="symbolic_optim_demo_eq92336.png" alt="$$5\tanh(x_1/5) \ge x_2^2 - 1.$$"></p><p>The constraints keep the optimization away from the global minimum point [1.333,1.037]. Visualize the two constraints:</p><pre class="codeinput">[X,Y] = meshgrid(-2:.01:3);
Z = (5*sinh(Y./5) &gt;= X.^4);
<span class="comment">% Z=1 where the first constraint is satisfied, Z=0 otherwise</span>
Z = Z+ 2*(5*tanh(X./5) &gt;= Y.^2 - 1);
<span class="comment">% Z=2 where the second is satisfied, Z=3 where both are</span>
surf(X,Y,Z,<span class="string">'LineStyle'</span>,<span class="string">'none'</span>);
set(gcf,<span class="string">'Color'</span>,<span class="string">'w'</span>) <span class="comment">% white background</span>
view(0,90)
hold <span class="string">on</span>
plot3(.4396, .0373, 4,<span class="string">'o'</span>,<span class="string">'MarkerEdgeColor'</span>,<span class="string">'r'</span>,<span class="string">'MarkerSize'</span>,8);
<span class="comment">% best point</span>
xlabel(<span class="string">'x'</span>);ylabel(<span class="string">'y'</span>);
hold <span class="string">off</span>
</pre><img vspace="5" hspace="5" src="symbolic_optim_demo_02.png" alt=""> <p>We plotted a small red circle around the optimal point.</p><p>Here is a plot of the objective function over the feasible region, the region that satisfies both constraints, pictured above in dark red, along with a small red circle around the optimal point:</p><pre class="codeinput">W = log(1 + 3*(Y - (X.^3 - X)).^2 + (X - 4/3).^2);
<span class="comment">% W = the objective function</span>
W(Z &lt; 3) = nan; <span class="comment">% plot only where the constraints are satisfied</span>
surf(X,Y,W,<span class="string">'LineStyle'</span>,<span class="string">'none'</span>);
view(68,20)
hold <span class="string">on</span>
plot3(.4396, .0373, .8152,<span class="string">'o'</span>,<span class="string">'MarkerEdgeColor'</span>,<span class="string">'r'</span>, <span class="keyword">...</span>
    <span class="string">'MarkerSize'</span>,8); <span class="comment">% best point</span>
xlabel(<span class="string">'x'</span>);ylabel(<span class="string">'y'</span>);zlabel(<span class="string">'z'</span>);
hold <span class="string">off</span>
</pre><img vspace="5" hspace="5" src="symbolic_optim_demo_03.png" alt=""> <p>The nonlinear constraints must be written in the form <tt>c(x) &lt;= 0</tt>. We compute all the symbolic constraints and their derivatives, and placethem in a function handle using <tt>matlabFunction</tt>.</p><p>The gradients of the constraints should be column vectors; they must beplaced in the objective function as a matrix, with each column of the matrix representing the gradient of one constraint function. This is the transpose of the form generated by <tt>jacobian</tt>, so we take the transpose below.</p><p>We place the nonlinear constraints into a function handle. <tt>fmincon</tt> expects the nonlinear constraints and gradients to be output in the order [c ceq gradc gradceq]. Since there are no nonlinear equality constraints,we output [ ] for <tt>ceq</tt> and <tt>gradceq</tt>.</p><pre class="codeinput">c1 = x1^4 - 5*sinh(x2/5);
c2 = x2^2 - 5*tanh(x1/5) - 1;
c = [c1 c2];
gradc = jacobian(c,x).'; <span class="comment">% transpose to put in correct form</span>
constraint = matlabFunction(c,[],gradc,[],<span class="string">'vars'</span>,{x});
</pre><p>The interior-point algorithm requires its Hessian function to be written as a separate function, instead of being part of the objective function. This is because a nonlinearly constrained function needs to include those constraints in its Hessian. Its Hessian is the Hessian of the Lagrangian; see the User's Guide for more information.</p><p>The Hessian function takes two input arguments: the position vector <tt>x</tt>, and the Lagrange multiplier structure lambda. The parts of the lambda structure that you use for nonlinear constraints are <tt>lambda.ineqnonlin</tt> and <tt>lambda.eqnonlin</tt>. For the current constraint, there are no linear equalities, so we use the two multipliers <tt>lambda.ineqnonlin(1)</tt> and <tt>lambda.ineqnonlin(2)</tt>.</p><p>We calculated the Hessian of the objective function in the first example. Now we calculate the Hessians of the two constraint functions, and make function handle versions with <tt>matlabFunction</tt>.</p><pre class="codeinput">hessc1 = jacobian(gradc(:,1),x); <span class="comment">% constraint = first c column</span>
hessc2 = jacobian(gradc(:,2),x);

hessfh = matlabFunction(hessf,<span class="string">'vars'</span>,{x});
hessc1h = matlabFunction(hessc1,<span class="string">'vars'</span>,{x});
hessc2h = matlabFunction(hessc2,<span class="string">'vars'</span>,{x});
</pre><p>To make the final Hessian, we put the three Hessians together, adding the appropriate Lagrange multipliers to the constraint functions.</p><pre class="codeinput">myhess = @(x,lambda)(hessfh(x) + <span class="keyword">...</span>
    lambda.ineqnonlin(1)*hessc1h(x) + <span class="keyword">...</span>
    lambda.ineqnonlin(2)*hessc2h(x));
</pre><p>Set the options to use the interior-point algorithm, the gradient, and the Hessian, have the objective function return both the objective and the gradient, and run the solver:</p><pre class="codeinput">options = optimset(<span class="string">'Algorithm'</span>,<span class="string">'interior-point'</span>,<span class="string">'GradObj'</span>,<span class="keyword">...</span>
    <span class="string">'on'</span>,<span class="string">'GradConstr'</span>,<span class="string">'on'</span>,<span class="string">'Hessian'</span>,<span class="string">'user-supplied'</span>,<span class="keyword">...</span>
    <span class="string">'HessFcn'</span>,myhess,<span class="string">'Display'</span>,<span class="string">'final'</span>);
<span class="comment">% fh2 = objective without Hessian</span>
fh2 = matlabFunction(f,gradf,<span class="string">'vars'</span>,{x});
[xfinal fval exitflag output] = fmincon(fh2,[-1;2],<span class="keyword">...</span>
    [],[],[],[],[],[],constraint,options)
</pre><pre class="codeoutput">
Local minimum found that satisfies the constraints.

Optimization completed because the objective function is non-decreasing in 
feasible directions, to within the default value of the function tolerance,
and constraints were satisfied to within the default value of the constraint tolerance.




xfinal =

   0.439569087135554
   0.037334020327260


fval =

   0.815247080686200


exitflag =

     1


output = 

         iterations: 10
          funcCount: 13
    constrviolation: 0
           stepsize: 1.916032009774339e-006
          algorithm: 'interior-point'
      firstorderopt: 1.921701378582164e-008
       cgiterations: 0
            message: [1x782 char]

</pre><p>Again, the solver makes many fewer iterations and function evaluations with gradient and Hessian supplied than when they are not:</p><pre class="codeinput">options = optimset(<span class="string">'Algorithm'</span>,<span class="string">'interior-point'</span>,<span class="keyword">...</span>
    <span class="string">'Display'</span>,<span class="string">'final'</span>);
<span class="comment">% fh3 = objective without gradient or Hessian</span>
fh3 = matlabFunction(f,<span class="string">'vars'</span>,{x});
<span class="comment">% constraint without gradient:</span>
constraint = matlabFunction(c,[],<span class="string">'vars'</span>,{x});
[xfinal fval exitflag output2] = fmincon(fh3,[-1;2],<span class="keyword">...</span>
    [],[],[],[],[],[],constraint,options)

sprintf([<span class="string">'There were %d iterations using gradient'</span> <span class="keyword">...</span>
    <span class="string">' and Hessian, but %d without them.'</span>],<span class="keyword">...</span>
    output.iterations,output2.iterations)
sprintf([<span class="string">'There were %d function evaluations using gradient'</span> <span class="keyword">...</span>
    <span class="string">' and Hessian, but %d without them.'</span>], <span class="keyword">...</span>
    output.funcCount,output2.funcCount)
</pre><pre class="codeoutput">
Local minimum found that satisfies the constraints.

Optimization completed because the objective function is non-decreasing in 
feasible directions, to within the default value of the function tolerance,
and constraints were satisfied to within the default value of the constraint tolerance.




xfinal =

   0.439568846240534
   0.037334303592384


fval =

   0.815247460667077


exitflag =

     1


output2 = 

         iterations: 17
          funcCount: 54
    constrviolation: 0
           stepsize: 4.241701937721570e-006
          algorithm: 'interior-point'
      firstorderopt: 3.843306486672748e-007
       cgiterations: 0
            message: [1x782 char]


ans =

There were 10 iterations using gradient and Hessian, but 17 without them.


ans =

There were 13 function evaluations using gradient and Hessian, but 54 without them.

</pre><h2>Cleaning Up Symbolic Variables<a name="14"></a></h2><p>The symbolic variables used in this demo were assumed to be real. To clear this assumption from the symbolic engine workspace, it is not sufficient to delete the variables. You must either clear the variables using the syntax</p><pre class="codeinput">syms <span class="string">x1</span> <span class="string">x2</span> <span class="string">clear</span>
</pre><p>or reset the symbolic engine using the command</p><pre>% reset(symengine) % uncomment this line to reset the engine</pre><p>After resetting the symbolic engine you should clear all symbolic variables from the MATLAB&reg; workspace:</p><pre>% clear % uncomment this line to clear the variables</pre><p class="footer">Copyright 1990-2010 The MathWorks, Inc.<br>
          Published with MATLAB&reg; 7.10</p><p class="footer" id="trademarks">MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  Please see <a href="http://www.mathworks.com/trademarks">www.mathworks.com/trademarks</a> for a list of other trademarks owned by The MathWorks, Inc.  Other product or brand names are trademarks or registered trademarks of their respective owners.</p></div><!--
##### SOURCE BEGIN #####
%% Using Symbolic Mathematics with Optimization Toolbox(TM) Solvers
%
% Optimization Toolbox(TM) solvers are usually more accurate and
% efficient when you supply gradients and Hessians of the
% objective and constraint functions. This demo shows how to use
% the Symbolic Math Toolbox(TM) functions named |jacobian| and
% |matlabFunction| to provide these derivatives to optimization
% solvers.
%
% *Additional Requirements:*
%
% * Symbolic Math Toolbox
%
% There are several considerations in using symbolic
% calculations with optimization functions:
%
% 1. Optimization objective and constraint functions should be
% defined in terms of a vector, say |x|. However, symbolic
% variables are scalar or complex-valued, not vector-valued.
% This requires you to translate between vectors and scalars.
%
% 2. Optimization gradients, and sometimes Hessians, are
% supposed to be calculated within the body of the objective or
% constraint functions. This means that a symbolic gradient or
% Hessian has to be placed in the appropriate place in the
% objective or constraint function file or function handle.
%
% 3. Calculating gradients and Hessians symbolically can be
% time-consuming. Therefore you should perform this calculation
% only once, and generate code, via |matlabFunction|, to call
% during execution of the solver.
%
% 4. Evaluating symbolic expressions with the |subs| function is
% time-consuming. It is much more efficient to use
% |matlabFunction|.
%
% 5. |matlabFunction| generates code that depends on the
% orientation of input vectors. Since |fmincon| calls the
% objective function with column vectors, you must be careful to
% call |matlabFunction| with column vectors of symbolic
% variables.

%   Copyright 1990-2010 The MathWorks, Inc.
%   $Revision: 1.1.6.3 $ $Date: 2009/11/05 17:01:12 $


%% First Example: Unconstrained Minimization with Hessian
%
% The objective function to minimize is:
%
% $$f(x_1, x_2) = \log\left (1 + 3 \left ( x_2 - (x_1^3 - x_1)
% \right )^2 + (x_1 - 4/3)^2 \right ).$$
%
% This function is positive, with a unique minimum value of zero
% attained at |x1| = 4/3, |x2| =(4/3)^3 - 4/3 = 1.0370...
%
% We write the independent variables as |x1| and |x2| because in
% this form they can be used as symbolic variables. As
% components of a vector |x| they would be written |x(1)| and
% |x(2)|. The function has a twisty valley as depicted in the
% plot below.

syms x1 x2 real
x = [x1;x2]; % column vector of symbolic variables
f = log(1 + 3*(x2 - (x1^3 - x1))^2 + (x1 - 4/3)^2);

ezsurfc(f,[-2 2])
view(127,38)

%%
% Compute the gradient and Hessian of f:

gradf = jacobian(f,x).'; % column gradf
hessf = jacobian(gradf,x);

%%
% The |fminunc| solver expects to pass in a vector x, and, with
% the |GradObj| and |Hessian| options set to 'on', expects a
% list of three outputs: [f(x),gradf(x),hessf(x)]
%
% |matlabFunction| generates exactly this list of three outputs
% from a list of three inputs. Furthermore, using the |vars|
% option, |matlabFunction| accepts vector inputs.

fh = matlabFunction(f,gradf,hessf,'vars',{x});

%%
% Now solve the minimization problem starting at the point
% [-1,2]:

options = optimset('GradObj','on','Hessian','on', ...
    'Display','final');
[xfinal fval exitflag output] = fminunc(fh,[-1;2],options)

%%
% Compare this with the number of iterations using no gradient
% or Hessian information. This requires the medium-scale
% algorithm, obtained by setting the |LargeScale| option to
% |'off'|:

options = optimset('Display','final','LargeScale','off');
fh2 = matlabFunction(f,'vars',{x}); 
% fh2 = objective with no gradient or Hessian
[xfinal fval exitflag output2] = fminunc(fh2,[-1;2],options)

%%
% The number of iterations is lower when using gradients and
% Hessians, and there are dramatically fewer function
% evaluations:

sprintf(['There were %d iterations using gradient' ...
    ' and Hessian, but %d without them.'], ...
    output.iterations,output2.iterations)
sprintf(['There were %d function evaluations using gradient' ...
    ' and Hessian, but %d without them.'], ...
    output.funcCount,output2.funcCount)

%% Second Example: Constrained Minimization Using the fmincon Interior-Point Algorithm
%
% We consider the same objective function and starting point,
% but now have two nonlinear constraints:
%
% $$5\sinh(x_2/5) \ge x_1^4$$
%
% $$5\tanh(x_1/5) \ge x_2^2 - 1.$$
%
% The constraints keep the optimization away from the global
% minimum point [1.333,1.037]. Visualize the two constraints:

[X,Y] = meshgrid(-2:.01:3);
Z = (5*sinh(Y./5) >= X.^4); 
% Z=1 where the first constraint is satisfied, Z=0 otherwise
Z = Z+ 2*(5*tanh(X./5) >= Y.^2 - 1); 
% Z=2 where the second is satisfied, Z=3 where both are
surf(X,Y,Z,'LineStyle','none');
set(gcf,'Color','w') % white background
view(0,90)
hold on
plot3(.4396, .0373, 4,'o','MarkerEdgeColor','r','MarkerSize',8); 
% best point
xlabel('x');ylabel('y');
hold off

%%
% We plotted a small red circle around the optimal point.
%
% Here is a plot of the objective function over the feasible
% region, the region that satisfies both constraints, pictured
% above in dark red, along with a small red circle around the
% optimal point:

W = log(1 + 3*(Y - (X.^3 - X)).^2 + (X - 4/3).^2); 
% W = the objective function
W(Z < 3) = nan; % plot only where the constraints are satisfied
surf(X,Y,W,'LineStyle','none');
view(68,20)
hold on
plot3(.4396, .0373, .8152,'o','MarkerEdgeColor','r', ...
    'MarkerSize',8); % best point
xlabel('x');ylabel('y');zlabel('z');
hold off

%%
% The nonlinear constraints must be written in the form |c(x)
% <= 0|. We compute all the symbolic constraints and their
% derivatives, and placethem in a function handle using
% |matlabFunction|.
%
% The gradients of the constraints should be column vectors;
% they must beplaced in the objective function as a matrix, with
% each column of the matrix representing the gradient of one
% constraint function. This is the transpose of the form
% generated by |jacobian|, so we take the transpose below.
%
% We place the nonlinear constraints into a function handle.
% |fmincon| expects the nonlinear constraints and gradients to
% be output in the order [c ceq gradc gradceq]. Since there are
% no nonlinear equality constraints,we output [ ] for |ceq| and
% |gradceq|.

c1 = x1^4 - 5*sinh(x2/5);
c2 = x2^2 - 5*tanh(x1/5) - 1;
c = [c1 c2];
gradc = jacobian(c,x).'; % transpose to put in correct form
constraint = matlabFunction(c,[],gradc,[],'vars',{x});

%%
% The interior-point algorithm requires its Hessian function to
% be written as a separate function, instead of being part of
% the objective function. This is because a nonlinearly
% constrained function needs to include those constraints in its
% Hessian. Its Hessian is the Hessian of the Lagrangian; see the
% User's Guide for more information.
%
% The Hessian function takes two input arguments: the position
% vector |x|, and the Lagrange multiplier structure lambda. The
% parts of the lambda structure that you use for nonlinear
% constraints are |lambda.ineqnonlin| and |lambda.eqnonlin|. For
% the current constraint, there are no linear equalities, so we
% use the two multipliers |lambda.ineqnonlin(1)| and
% |lambda.ineqnonlin(2)|.
%
% We calculated the Hessian of the objective function in the
% first example. Now we calculate the Hessians of the two
% constraint functions, and make function handle versions with
% |matlabFunction|.

hessc1 = jacobian(gradc(:,1),x); % constraint = first c column
hessc2 = jacobian(gradc(:,2),x);

hessfh = matlabFunction(hessf,'vars',{x});
hessc1h = matlabFunction(hessc1,'vars',{x});
hessc2h = matlabFunction(hessc2,'vars',{x});

%%
% To make the final Hessian, we put the three Hessians together,
% adding the appropriate Lagrange multipliers to the constraint
% functions.

myhess = @(x,lambda)(hessfh(x) + ...
    lambda.ineqnonlin(1)*hessc1h(x) + ...
    lambda.ineqnonlin(2)*hessc2h(x));

%%
% Set the options to use the interior-point algorithm, the
% gradient, and the Hessian, have the objective function return
% both the objective and the gradient, and run the solver:

options = optimset('Algorithm','interior-point','GradObj',...
    'on','GradConstr','on','Hessian','user-supplied',...
    'HessFcn',myhess,'Display','final');
% fh2 = objective without Hessian
fh2 = matlabFunction(f,gradf,'vars',{x});
[xfinal fval exitflag output] = fmincon(fh2,[-1;2],...
    [],[],[],[],[],[],constraint,options)
%%
% Again, the solver makes many fewer iterations and function
% evaluations with gradient and Hessian supplied than when they
% are not:

options = optimset('Algorithm','interior-point',...
    'Display','final');
% fh3 = objective without gradient or Hessian
fh3 = matlabFunction(f,'vars',{x});
% constraint without gradient:
constraint = matlabFunction(c,[],'vars',{x});
[xfinal fval exitflag output2] = fmincon(fh3,[-1;2],...
    [],[],[],[],[],[],constraint,options)

sprintf(['There were %d iterations using gradient' ...
    ' and Hessian, but %d without them.'],...
    output.iterations,output2.iterations)
sprintf(['There were %d function evaluations using gradient' ...
    ' and Hessian, but %d without them.'], ...
    output.funcCount,output2.funcCount)

%% Cleaning Up Symbolic Variables
%
% The symbolic variables used in this demo were assumed to be
% real. To clear this assumption from the symbolic engine
% workspace, it is not sufficient to delete the variables. You
% must either clear the variables using the syntax

syms x1 x2 clear

%%
% or reset the symbolic engine using the command
%
%  % reset(symengine) % uncomment this line to reset the engine
%
% After resetting the symbolic engine you should clear all
% symbolic variables from the MATLAB(R) workspace:
%
%  % clear % uncomment this line to clear the variables

displayEndOfDemoMessage(mfilename)

##### SOURCE END #####
--></body></html>